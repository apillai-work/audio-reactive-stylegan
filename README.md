# Audio Reactive StyleGAN

Generative art ensures the effective utilization of everything that technology has to offer, culminating in beautiful and evocative creations – some of which expanding over the same motivations that artists have pursued ever since the rise in the popularity of modern art. A quick scan through the internet could result in one finding a gigantic amount of new artworks being posted online almost every day, hence exploring various techniques that could be utilised to create such artworks is quite relevant presently. Lucid Sonic Dreams, a python package that helps users sync GAN Art to music, is one of the most easiest and accessible tools currently available to make mesmerising artworks that are audio-reactive, whilst giving users quite a lot of creative control over the visuals via over 30 parameter-options. The purpose of this project was to explore a track from two genres – Rock and Experimental Music, and see how the visuals generated would look like. The first part of the project (Rock Music) focused more on visuals and utilised three different styes for the artwork, whereas the second part (Experimental Music) focused more on sound source separation and seeing if the outputs generated were better using this method.

The first part used ‘Burning Down the House’ by Talking Heads as the input audio for the artwork. For the first output, WikiArt style and default settings were used. The end result was quite pleasant, although a lot was going on and some of the visuals weren’t exactly in sync with the drums / vocals. For the second output, HQ Flickr Faces were used as the style, and more parameters were added. The speed_fpm value was kept at 12 to avoid too many changes between the images, whilst letting the Pulse reactive to the percussive elements of the audio track. The output generated looks pleasant, although again the audio doesn’t seem to be in sync with the music at certain times. The last output, the maps style was used and contrast-strength and flash_strength parameters were added to try and slightly distinguish the changes in the visual. The out looked nice but felt a bit repetitive after a while, perhaps due to the nature of the style dataset.

The second part used ‘Ponyboy’ by SOPHIE as the input audio for the artwork. The style used for all the outputs were the same – ‘my little pony’ to keep things consistent. The default settings were used for the first output (sophie.mp4) and the results were surprisingly very good, possibly because of drum-usage in this track. Then, sound source separation was done to separate out components such as vocals, drums, bass, and other (melody) using a machine learning tool called demucs. The second output yielded good results too, but the third output was very dull (possibly because demucs couldn’t properly tell the different between the vocals at the beginning and the drums).


